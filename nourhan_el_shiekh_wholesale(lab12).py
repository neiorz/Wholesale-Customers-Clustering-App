# -*- coding: utf-8 -*-
"""Nourhan_El-shiekh_Wholesale(lab12).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16nXNLyG9dKXwIoL49q-vkBEek0R6Shm5

## Import required libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

"""## Load the dataset"""

data = pd.read_csv("/content/Wholesale customers data.csv")

data.head()

data.describe().transpose()

data.info()

data.shape

data.isna().sum().sum()

"""## Histograms for each numerical column"""

data.hist(bins=30, figsize=(15,10))
plt.suptitle("Histograms of Wholesale Customers Data Features")
plt.show()

"""## Correlation heatmap"""

plt.figure(figsize=(10,6))
sns.heatmap(data.corr(), annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Correlation Heatmap of Features")
plt.show()

"""## Data Cleaning

## Boxplots to detect outliers for each feature
"""

plt.figure(figsize=(15,10))
for i, column in enumerate(data.columns[2:], 1):  # skip Channel, Region (categorical)
    plt.subplot(2, 3, i)
    sns.boxplot(y=data[column])
    plt.title(f"Boxplot of {column}")
plt.tight_layout()
plt.show()

"""## Log transformation to handle skewness"""

log_data = data.copy()
for column in data.columns[2:]:  # skip Channel, Region
    log_data[column] = np.log1p(data[column])  # log1p(x) = log(x+1) to avoid log(0)

"""## Compare distributions before and after transformation"""

plt.figure(figsize=(15,10))
for i, column in enumerate(data.columns[2:], 1):
    plt.subplot(2, 3, i)
    sns.histplot(log_data[column], bins=30, kde=True)
    plt.title(f"Log-transformed {column}")
plt.tight_layout()
plt.show()

"""## Outlier Treatment (IQR method)"""

def cap_outliers(df, column):
    """Cap outliers in a column using IQR method"""
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # Cap values outside the range
    df[column] = np.where(df[column] < lower_bound, lower_bound,
                          np.where(df[column] > upper_bound, upper_bound, df[column]))
    return df

# Make a copy to work on
cleaned_data = data.copy()

# Apply capping on numerical columns only
for col in cleaned_data.columns[2:]:  # skip Channel, Region
    cleaned_data = cap_outliers(cleaned_data, col)

# Check boxplots again after capping
plt.figure(figsize=(15,10))
for i, column in enumerate(cleaned_data.columns[2:], 1):
    plt.subplot(2, 3, i)
    sns.boxplot(y=cleaned_data[column])
    plt.title(f"Boxplot of {column} (Capped)")
plt.tight_layout()
plt.show()

"""## Prepare Data for Clustering"""

# Drop categorical columns (Channel, Region)
cluster_data = cleaned_data.drop(["Channel", "Region"], axis=1)

# Apply log transformation to reduce skewness
log_cluster_data = np.log1p(cluster_data)  # log(x+1) to handle zeros

"""## Standardize the data (important for clustering)"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaled_cluster_data = scaler.fit_transform(log_cluster_data)

# Convert back to DataFrame for readability
scaled_cluster_data = pd.DataFrame(scaled_cluster_data, columns=cluster_data.columns)

print("Data is now cleaned, log-transformed, and scaled. Ready for clustering!")
print(scaled_cluster_data.head())

"""## Visualize Data Before Clustering (PCA)"""

from sklearn.decomposition import PCA

# Reduce dimensions to 2D for visualization
pca = PCA(n_components=2)
pca_data = pca.fit_transform(scaled_cluster_data)

# Put into DataFrame
pca_df = pd.DataFrame(data=pca_data, columns=["PC1", "PC2"])

# Plot PCA result
plt.figure(figsize=(8,6))
sns.scatterplot(x="PC1", y="PC2", data=pca_df, alpha=0.7)
plt.title("PCA Projection of Customers (Before Clustering)")
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.show()

"""##Modeling

## K-Means Clustering - Find Optimal K
"""

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

inertia = []
silhouette_scores = []
K = range(2, 11)  # test cluster sizes from 2 to 10

for k in K:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(scaled_cluster_data)
    inertia.append(kmeans.inertia_)
    silhouette_scores.append(silhouette_score(scaled_cluster_data, kmeans.labels_))

"""## Plot Elbow Method (Inertia)"""

plt.figure(figsize=(10,5))
plt.plot(K, inertia, marker='o')
plt.title("Elbow Method - Inertia vs Number of Clusters")
plt.xlabel("Number of Clusters (k)")
plt.ylabel("Inertia")
plt.show()

"""## Plot Silhouette Scores"""

plt.figure(figsize=(10,5))
plt.plot(K, silhouette_scores, marker='o')
plt.title("Silhouette Score vs Number of Clusters")
plt.xlabel("Number of Clusters (k)")
plt.ylabel("Silhouette Score")
plt.show()

"""## Final K-Means Clustering (k=3)"""

final_kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
clusters = final_kmeans.fit_predict(scaled_cluster_data)

# Add cluster labels to the original data
clustered_data = cleaned_data.copy()
clustered_data["Cluster"] = clusters

print(clustered_data.head())

"""##Visualize Clusters with PCA"""

# Get the cluster centers in scaled space
centers = final_kmeans.cluster_centers_

# Apply the same PCA transformation
centers_pca = pca.transform(centers)

#  Add cluster labels to the PCA DataFrame
pca_df["Cluster"] = final_kmeans.labels_

# Plot clusters
plt.figure(figsize=(8,6))
sns.scatterplot(x="PC1", y="PC2", hue="Cluster", palette="Set1", data=pca_df, alpha=0.7)

# Plot the centroids
plt.scatter(centers_pca[:,0], centers_pca[:,1],
            c="black", s=250, marker="X", label="Centroids")

plt.title("K-Means Clustering (k=3) - PCA Projection with Centroids")
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.legend(title="Cluster")
plt.show()

"""##Analyze Cluster Characteristics"""

cluster_means = clustered_data.groupby("Cluster").mean()
print("Cluster Profiles:")
print(cluster_means)

import joblib
joblib.dump(final_kmeans, "model.pkl")

